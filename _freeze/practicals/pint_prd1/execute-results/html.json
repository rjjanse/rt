{
  "hash": "1da596f50db4a63f0193f3d2bd0deea1",
  "result": {
    "markdown": "---\ntitle: \"PINT - Prediction Practical 1\"\nsubtitle: \"Analysis and Reporting of Prediction Modelling Studies\"\nauthor: \"Roemer J. Janse\"\ndate: today\nformat:\n    html:\n        theme: flatly\n        toc: true\n        number-sections: true\n        code-line-numbers: true\n        other-links:\n        - text: EPI-B\n          href: https://www.lumc.nl/en/afdelingen/Clinical-Epidemiology/epidemioloog-b/\n          target: _blank\n        code-links:\n        - text: Source \n          icon: file-code\n          href: pint-prd1.qmd\n---\n\n\n# Preface\n\nThis R practical will discuss the necessities for prediction modelling in R. However, we assume some basic knowledge on how R works. If you are new to R, we suggest you take a look at [this](https://rjjanse.github.io/rt) freely available R tutorial. Following the tutorial up until the section 'dplyr' should be enough to understand this practical.\n\n# Introduction\n\nIn this practical, we will see what are the minimal requirements to develop a prediction model in R (i.e. not from a statistical point of view) and develop a prediction model ourselves.\n\nAt the end of this practical, you will know what is required to develop and validate a prediction model in R. Additionally, you will understand the differences between different packages for prediction modelling.\n\n# Setting up R\n\nBefore we get into the fun part, of course we have to load a number of packages that will help us.\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Loading packages\npacman::p_load(\"dplyr\",         # Data wrangling\n               \"magrittr\",      # More efficient pipelines\n               \"rio\",           # Importing data\n               \"DescTools\",     # C-statistic for logistic models\n               \"pROC\",          # C-statistic for logistic models\n               \"rms\",           # Developing prediction models and restricted cubic splines\n               \"survival\",      # Developing Cox prediction models\n               \"splines\",       # Natural cubic splines\n               \"mice\",          # Multiple imputation of data\n               \"ggplot2\",       # Data visualization\n               \"ggExtra\"        # Add-ons for ggplot2\n)\n```\n:::\n\n\n::: column-margin\nNote that we use [`{pacman}`](https://rjjanse.github.io/rt/packages.html#pacman) instead of the usual approach with `install.packages()` and `library()`, but that this requires a one-time prior installation of `{pacman}`:\n\n``` r\ninstall.packages(\"pacman\")\n```\n:::\n\n# Loading data\n\nFor this practical, we will use two datasets: \\* Traumatic brain injury (TBI) data This dataset contains 2,159 patients from the international and US Tirilazad trials (distributed here for didactic purposes only). The primary outcome was the Glasgow Outcome (range 1 through 5) at 6 months. \\* NCCTG lung cancer data The NCCTG lung cancer dataset contains 228 patients from the North Central Cancer Treatment group, with information on time until death and performance scores.\n\nThe TBI dataset was sent to you together with this practical and can be loaded from your local device:\n\n``` r\n# Specify path of TBI dataset\npath <- \"C:/users/avid_PINT_student/documents/pint/TBI.txt\"\n\n# Load TBI data\ntbi <- import(path)\n```\n\n\n::: {.cell}\n::: {.cell-output .cell-output-stderr}\n```\nWarning in (function (input = \"\", file = NULL, text = NULL, cmd = NULL, :\nDetected 24 column names but the data has 25 columns (i.e. invalid file). Added\n1 extra default column name for the first column which is guessed to be row\nnames or an index. Use setnames() afterwards if this guess is not correct, or\nfix the file write command that created the file to create a valid file.\n```\n:::\n:::\n\n\nThe warning we receive is the result of row numbers being present in the .txt file (as also guessed by `import()`). Given that `import()` guessed correct, we can ignore this warning and treat the extra column V1 as individual identifiers.\n\nThe NCCTG lung cancer dataset is provided by the `{survival}` package and can be loaded from within R:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlung <- lung\n```\n:::\n\n\n::: column-margin\nMore formally, we could call the lung dataset with:\n\n``` r\ndata(cancer)\n```\n\nHowever, this will show all related cancer datasets available in `{survival}` as promises (i.e. available to you but not loaded into memory). If you run lung once, it will be loaded into your global environment:\n\n``` r\nlung\n```\n\nNonetheless, the other datasets remain visible as promises in your global environment, which can be rather messy.\n:::\n\nBelow are the codebooks for both the TBI and the lung data.\n\n+--------------+----------------------------------------+--------------+------------------------------------------------------------------+\n|              | **TBI**                                |              | **Lung**                                                         |\n+--------------+----------------------------------------+--------------+------------------------------------------------------------------+\n| **Variable** | **Description**                        | **Variable** | **Description**                                                  |\n+--------------+----------------------------------------+--------------+------------------------------------------------------------------+\n| V1           | Identifier                             | inst         | Institution code                                                 |\n+--------------+----------------------------------------+--------------+------------------------------------------------------------------+\n| trial        | Trial identification                   | time         | Survival time (*days*)                                           |\n+--------------+----------------------------------------+--------------+------------------------------------------------------------------+\n| d.gos        | Glasgow Outcome Scale at 6 months      | status       | Censoring status:                                                |\n|              |                                        |              |                                                                  |\n|              | 1.  dead                               |              | 1.  Censored                                                     |\n|              | 2.  vegetative                         |              | 2.  Dead                                                         |\n|              | 3.  severe disability                  |              |                                                                  |\n|              | 4.  moderate disability                |              |                                                                  |\n|              | 5.  good recovery                      |              |                                                                  |\n+--------------+----------------------------------------+--------------+------------------------------------------------------------------+\n| d.mort       | Mortality at 6 months                  | age          | Age (*yrs*)                                                      |\n+--------------+----------------------------------------+--------------+------------------------------------------------------------------+\n| d.unfav      | Unfavourable outcome at 6 months       | sex          | Biological sex                                                   |\n|              |                                        |              |                                                                  |\n|              |                                        |              | 1.  Male                                                         |\n|              |                                        |              | 2.  Female                                                       |\n+--------------+----------------------------------------+--------------+------------------------------------------------------------------+\n| cause        | Cause of injury                        | ph.ecog      | ECOG performance score as rated by the physician                 |\n|              |                                        |              |                                                                  |\n|              | 3.  road traffic accident              |              | 0.  asymptomatic                                                 |\n|              | 4.  motorbike                          |              | 1.  symptomatic but completely ambulatory                        |\n|              | 5.  assault                            |              | 2.  in bed \\<50% of the day                                      |\n|              | 6.  domestic/fall                      |              | 3.  in bed \\>50% of the day but not bedbound                     |\n|              | 7.  other                              |              | 4.  bedbound                                                     |\n+--------------+----------------------------------------+--------------+------------------------------------------------------------------+\n| age          | Age (*yrs*)                            | ph.karno     | Karnofsky performance score rated by the physician (range 0-100) |\n+--------------+----------------------------------------+--------------+------------------------------------------------------------------+\n| d.motor      | Admission motor score (range 1-6)      | pat.karno    | Karnofsky performance score as rated by patient                  |\n+--------------+----------------------------------------+--------------+------------------------------------------------------------------+\n| d.pupil      | Pupillary reactivity                   | meal.cal     | Calories consumed at meals                                       |\n|              |                                        |              |                                                                  |\n|              | 1.  both reactive                      |              |                                                                  |\n|              | 2.  one reactive                       |              |                                                                  |\n|              | 3.  none reactive                      |              |                                                                  |\n+--------------+----------------------------------------+--------------+------------------------------------------------------------------+\n| pupil.i      | Single imputed pupillary reactivity    | wt.loss      | Weight loss in last six months (*pounds*)                        |\n+--------------+----------------------------------------+--------------+------------------------------------------------------------------+\n| hypoxia      | Hypoxia before or at admission         |              |                                                                  |\n+--------------+----------------------------------------+--------------+------------------------------------------------------------------+\n| hypotens     | Hypotension before or at admission     |              |                                                                  |\n+--------------+----------------------------------------+--------------+------------------------------------------------------------------+\n| ctclass      | Marshall CT classification (range 1-6) |              |                                                                  |\n+--------------+----------------------------------------+--------------+------------------------------------------------------------------+\n| tsah         | tSAH at CT                             |              |                                                                  |\n+--------------+----------------------------------------+--------------+------------------------------------------------------------------+\n| edh          | EDH at CT                              |              |                                                                  |\n+--------------+----------------------------------------+--------------+------------------------------------------------------------------+\n| cisterns     | Compressed cisterns at CT              |              |                                                                  |\n|              |                                        |              |                                                                  |\n|              | 0.  no                                 |              |                                                                  |\n|              | 1.  slightly                           |              |                                                                  |\n|              | 2.  fully                              |              |                                                                  |\n+--------------+----------------------------------------+--------------+------------------------------------------------------------------+\n| shift        | Midline shift \\>5 mm at CT             |              |                                                                  |\n+--------------+----------------------------------------+--------------+------------------------------------------------------------------+\n| glucose      | Glucose at admission (*mmol/L*)        |              |                                                                  |\n+--------------+----------------------------------------+--------------+------------------------------------------------------------------+\n| glucoset     | Truncated glucose values (*mmol/L*)    |              |                                                                  |\n+--------------+----------------------------------------+--------------+------------------------------------------------------------------+\n| ph           | pH                                     |              |                                                                  |\n+--------------+----------------------------------------+--------------+------------------------------------------------------------------+\n| sodium       | Sodium (*mmol/L*)                      |              |                                                                  |\n+--------------+----------------------------------------+--------------+------------------------------------------------------------------+\n| hb           | Hemoglobin (*g/dL*)                    |              |                                                                  |\n+--------------+----------------------------------------+--------------+------------------------------------------------------------------+\n| hbt          | Truncated hemoglobin (*g/dL*)          |              |                                                                  |\n+--------------+----------------------------------------+--------------+------------------------------------------------------------------+\n\n: Codebooks for the TBI and lung datasets {.striped .hover .sm}\n\n# Predicting an unfavourable outcome\nWe will develop a prediction model for the risk of an unfavourable outcome after a TBI. As a first step, let's better understand our data.\n\n## Getting to know our data\nThe degree to which a prediction model is likely not overfitted depends in part on the number of cases and non-cases:\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Counts of non-cases and cases\ntable(tbi[[\"d.unfav\"]])\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n\n   0    1 \n1308  851 \n```\n:::\n\n```{.r .cell-code}\n# Distribution of non-cases and cases (as %)\nproportions(table(tbi[[\"d.unfav\"]])) * 100\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n\n      0       1 \n60.5836 39.4164 \n```\n:::\n:::\n\n\nIn the TBI dataset, we can see that we have quite a lot of cases (n = 851, 39.4%).\n\nWe (hypothetically) consulted some TBI experts, who told us that the most important predictors we want to include are motor activity ('d.motor'), pupillary reactivity ('d.pupil'), TBI cause ('cause'), and age ('age'). Although we will estimate quite a lot of coefficients (12) because motor activity, pupillary reactivity, and cause are categorical, we also have quite some events so we are not too worried about overfitting.\n\nBecause age is continuous, we should investigate whether adding it as a linear term is sufficient or if we model it non-linearly. We can do this by modelling the outcome as a function of age and visually inspecting the results.\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Model the outcome as a function of age\nfit <- glm(d.unfav ~ age, data = tbi, family = binomial)         # <1>\n\n# Plot predicted probabilities vs. age\nggplot(data = tbi, aes(x = age, y = fit[[\"fitted.values\"]])) +   # <2>\n    # Geometries\n    geom_point() +                                               # <3>\n    # Labels\n    xlab(\"Age (yrs)\") + ylab(\"Predicted probability\") +          # <4>\n    # Aesthetics\n    theme_bw()                                                   # <5>\n```\n\n::: {.cell-output-display}\n![](pint_prd1_files/figure-html/unnamed-chunk-5-1.png){width=672}\n:::\n:::\n\n\n1. Fit a logistic regression model with the outcome 'd.unfav' as a function of age.\n2. Create the basis for a plot, where age is on the x-axis and the fitted values from the logistic regression model are on the y-axis.\n3. Draw points in the plot based on the data.\n4. Change the axes labels.\n5. Change the look of the plot to a pre-specified theme.\n\nFrom the figure, we can see that age is linearly associated with the outcome, so we do not need to apply any further transformation.\n\n## Method I: glm()\n### Developing the prediction model\nWe can now develop our prediction model. Because the outcome is binary, we will use a logistic regression model. Because a logistic regression model is part of the generalized linear models (GLM) family, we can use the `glm()` function as already done above. We specify that the family (i.e. the distribution of the data) is binomial (0 or 1), which specifies that we are fitting a logistic model.\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Develop the prediction model\nfit <- glm(d.unfav ~                                             # <1>\n               age +                                             # <2>\n               as.factor(d.motor) +                              # <3>\n               as.factor(d.pupil) +\n               as.factor(cause), \n           data = tbi, family = binomial)                        # <4>\n```\n:::\n\n\n1. Fit a logistic regression model with the outcome 'd.unfav'.\n2. Add age as a normal predictor (because we saw it was linearly associated with the outcome).\n3. Add the categorical predictors as factors (to make sure R recognizes them as categories).\n4. Specify that the data source and the family to define a logistic model.\n\nLet's see what the output looks like:\n\n::: {.cell}\n\n```{.r .cell-code}\n# Print output\nsummary(fit)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n\nCall:\nglm(formula = d.unfav ~ age + as.factor(d.motor) + as.factor(d.pupil) + \n    as.factor(cause), family = binomial, data = tbi)\n\nCoefficients:\n                                       Estimate Std. Error z value Pr(>|z|)    \n(Intercept)                           -1.633692   0.660303  -2.474 0.013355 *  \nage                                    0.037503   0.004037   9.289  < 2e-16 ***\nas.factor(d.motor)2                    0.648468   0.621264   1.044 0.296583    \nas.factor(d.motor)3                    0.057519   0.614843   0.094 0.925466    \nas.factor(d.motor)4                   -0.660139   0.609902  -1.082 0.279089    \nas.factor(d.motor)5                   -1.304401   0.610827  -2.135 0.032723 *  \nas.factor(d.motor)6                   -1.628588   0.680168  -2.394 0.016648 *  \nas.factor(d.pupil)no reactive pupils   1.285761   0.148476   8.660  < 2e-16 ***\nas.factor(d.pupil)one reactive         0.552311   0.148264   3.725 0.000195 ***\nas.factor(cause)domestic/fall          0.326742   0.247825   1.318 0.187357    \nas.factor(cause)Motorbike              0.162727   0.245626   0.662 0.507652    \nas.factor(cause)other                  0.435268   0.245729   1.771 0.076505 .  \nas.factor(cause)Road traffic accident  0.183664   0.231363   0.794 0.427292    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n(Dispersion parameter for binomial family taken to be 1)\n\n    Null deviance: 2726.7  on 2035  degrees of freedom\nResidual deviance: 2251.5  on 2023  degrees of freedom\n  (123 observations deleted due to missingness)\nAIC: 2277.5\n\nNumber of Fisher Scoring iterations: 4\n```\n:::\n:::\n\n\nThe attentive reader might notice that in one of the last lines, the following message is printed:\n\n::: {.cell}\n::: {.cell-output .cell-output-stdout}\n```\n(123 observations deleted due to missingness)\n```\n:::\n:::\n\n\nIt is important to realize that, like much statistical software, individuals with missing values are excluded by default. However, if the missingness is informative (which is often the case), this likely results in a biased prediction model (and we lose some power). Luckily, the TBI data is supplied with the variable 'pupil.i'. This is a single imputed version of 'd.pupil'. Although single imputation is often unsatisfactory, for our current didactic purposes it is enough.\n\nIf we refit the model with the imputed variable, we get:\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Develop the prediction model\nfit <- glm(d.unfav ~                                             # <1>\n               age +                                             # <2>\n               as.factor(d.motor) +                              # <3>\n               as.factor(pupil.i) +\n               as.factor(cause), \n           data = tbi, family = binomial)                        # <4>\n\n# Print output\nsummary(fit)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n\nCall:\nglm(formula = d.unfav ~ age + as.factor(d.motor) + as.factor(pupil.i) + \n    as.factor(cause), family = binomial, data = tbi)\n\nCoefficients:\n                                       Estimate Std. Error z value Pr(>|z|)    \n(Intercept)                           -1.462684   0.643773  -2.272  0.02308 *  \nage                                    0.037715   0.003887   9.703  < 2e-16 ***\nas.factor(d.motor)2                    0.482682   0.607106   0.795  0.42658    \nas.factor(d.motor)3                   -0.068894   0.601206  -0.115  0.90877    \nas.factor(d.motor)4                   -0.739708   0.596682  -1.240  0.21509    \nas.factor(d.motor)5                   -1.378086   0.597590  -2.306  0.02111 *  \nas.factor(d.motor)6                   -1.784168   0.667584  -2.673  0.00753 ** \nas.factor(pupil.i)no reactive pupils   1.270588   0.142312   8.928  < 2e-16 ***\nas.factor(pupil.i)one reactive         0.578122   0.142276   4.063 4.84e-05 ***\nas.factor(cause)domestic/fall          0.182092   0.235049   0.775  0.43852    \nas.factor(cause)Motorbike              0.075287   0.233211   0.323  0.74683    \nas.factor(cause)other                  0.356963   0.232921   1.533  0.12539    \nas.factor(cause)Road traffic accident  0.117619   0.219094   0.537  0.59138    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n(Dispersion parameter for binomial family taken to be 1)\n\n    Null deviance: 2895.5  on 2158  degrees of freedom\nResidual deviance: 2410.3  on 2146  degrees of freedom\nAIC: 2436.3\n\nNumber of Fisher Scoring iterations: 4\n```\n:::\n:::\n\n\nThe message on observations removed due to missingness is now gone (because no individuals were removed).\n\n### Validating the prediction model\nNow that we have a prediction model, we have to make sure it actually works. To do this, we compare the observed and the predicted values. To this end, we will have to somehow get the predicted values from the prediction model.\n\nThere are two easy ways to do that:\n\n::: {.cell}\n\n```{.r .cell-code}\n# Get predicted values from model fit object\ntbi[[\"preds\"]] <- fit[[\"fitted.values\"]]              # <1>\n\n# Get predicted values from function\npreds_fun <- predict(fit, type = \"response\")     # <2>\n\n# Compare values\ntable(tbi[[\"preds\"]] == preds_fun)                    # <3>\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n\nTRUE \n2159 \n```\n:::\n:::\n\n\n\n1. The predicted values are already stored in the model fit object. This makes it easy for us to extract and use those values.\n\n2. We can also use the predict function. The predict function contains many subclasses that recognize what type of model we fitted. In this case, R automatically detects it should use the subclass `predict.glm()`. By defining 'response', we tell R that we want the predicted probabilities. The advantage of `predict()` is that we can also supply the `newdata` argument. By supplying `newdata` with a data frame with column names corresponding to the predictors, the `predict()` function calculates predictions for this new data.\n\n3. We can see that the predictions are the same for each individual.\n\nWith these predictions, we can calculate our C-statistic:\n\n::: {.cell}\n\n```{.r .cell-code}\n# Calculate C-statistic with the Cstat() function from the {DescTools} package.\nCstat(fit)                              \n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] 0.7651261\n```\n:::\n:::\n\n\nTo calculate our calibration intercept and calibration slope, we will need the linear predictors.\n\n::: {.cell}\n\n```{.r .cell-code}\n# Get linear predictors from model fit for calibration intercept and slope\ntbi[[\"lps\"]] <- fit[[\"linear.predictors\"]]                                    # <1>\n\n# Calculate calibration intercept and slope \nfit_lps <- glm(d.unfav ~ lps, data = tbi, family = binomial)                  # <2>\n\n# Get intercept and slope\ncat(\"Intercept:\", format(round(fit_lps[[\"coefficients\"]][[\"(Intercept)\"]], 3), nsmall = 3))   # <3>\ncat(\"\\nSlope:\", format(round(fit_lps[[\"coefficients\"]][[\"lps\"]], 3), nsmall = 3))\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nIntercept: 0.000\nSlope: 1.000\n```\n:::\n:::\n\n\n1. As with getting the predicted values, we can use the model fit, or use the `predict()` function with `type = \"link\"`, which has the advantage of allowing new data.\n\n2. We refit the regression model used to develop our prediction model with the outcome as a function of the linear predictors.\n\n3. Print the intercept and the slope (i.e. the regression coefficient for the linear predictors). `round()` rounds the values and `format()` prints the decimals up to the value supplied in `nsmall`, even if they are only 0's.\n\nFinally, we can make our calibration plot:\n\n::: {.cell}\n\n```{.r .cell-code}\n# Create calibration plot\nggplot(tbi, aes(x = preds, y = d.unfav)) +                 # <1>\n    # Geometries\n    geom_abline(linewidth = 1, colour = \"black\", alpha = 0.33) +   # <2>\n    geom_point(alpha = 0.33) + # <3> \n    geom_smooth(colour = \"#785EF0\", fill = \"#785EF0\", method = \"loess\", formula = \"y ~ x\") +  # <4>\n    # Labels                           # <5>    \n    xlab(\"Predicted probability\") +            \n    ylab(\"Observed probability\") +\n    # Aesthetics                            \n    theme_bw() # <6>\n```\n\n::: {.cell-output-display}\n![](pint_prd1_files/figure-html/unnamed-chunk-13-1.png){width=672}\n:::\n:::\n\n\n1. We create a base layer with the data for the calibration plot.\n2. We add the harmonisation line that shows perfect calibration, which is 67% transparent (`alpha = 0.33`).\n3. We draw points on the plot that represent the predicted vs. observed values for each individual.\n4. Because we have this data, we can also draw a locally estimated scatterplot smoothing (LOESS) curve which shows the calibration over the range of predicted values.\n5. We define the labels for the axes.\n6. We change the look of the plot based on a prespecified theme.\n\nWe can also add the histogram of predicted probabilities using `ggMarginal()` from `{ggExtras}`.\n\n::: {.cell}\n\n```{.r .cell-code}\n# Create calibration plot\np <- ggplot(tbi, aes(x = preds, y = d.unfav, colour = as.factor(d.unfav))) +                 # <1>\n    # Geometries\n    geom_abline(linewidth = 1, colour = \"black\", alpha = 0.33) +   \n    geom_point(alpha = 0.33) + # <3>\n    geom_smooth(colour = \"#785EF0\", fill = \"#785EF0\", method = \"loess\", formula = \"y ~ x\") +  \n    # Scaling\n    scale_colour_manual(values = c(\"#648FFF\", \"#FFB000\"),             # <2>\n                        labels = c(\"Favourable outcome\", \"Unfavourable outcome\")) +\n    # Labels                               \n    xlab(\"Predicted probability\") +            \n    ylab(\"Observed probability\") +\n    # Aesthetics                            \n    theme_bw() +\n    theme(legend.position = \"bottom\",     # <3>\n          legend.title = element_blank())  \n\n# Add histograms\nggMarginal(p, type = \"histogram\", margins = \"x\", binwidth = 0.01, groupFill = TRUE) # <4>\n```\n\n::: {.cell-output-display}\n![](pint_prd1_files/figure-html/unnamed-chunk-14-1.png){width=672}\n:::\n:::\n\n\n1. To have two separately coloured histograms for individuals with and without the outcome, we need to specify\non what variable to stratify colour. As a side effect, this also colours the points drawn with `geom_point()`.\n2. With a manual scale, we define the colours and labels corresponding to each colour.\n3. We slightly adjust the theme by moving the legend to the bottom of the plot and removing the title of the legend.\n4. We add the histograms to the plot.\n\n## Method II: lrm()\nWe now used the `glm()` function from one of the base R packages (`{stats}`). However, there is another package that is often used to develop prediction models: `{rms}`.\n\n### Developing the prediction model\nTo develop our prediction model using `{rms}`, we use the function `lrm()` for a logistic regression model.\n\n::: {.cell}\n\n```{.r .cell-code}\n# First, set numeric variables to factor (lrm() does not work with as.factor())\ntbi[[\"d.motor\"]] <- as.factor(tbi[[\"d.motor\"]])\n\n# Develop the prediction model\nfit_rms <- lrm(d.unfav ~                        \n                   age +                        \n                   d.motor +         \n                   pupil.i +\n                   cause, \n               data = tbi, x = TRUE, y = TRUE)  # <1>                   \n```\n:::\n\n\n1. We have to specify x and y as TRUE to use the functions for validation in `{rms}`.\n\n### Validating the prediction model\nThe `{rms}` package offers an easy function validate prediction models: `validate()`.\n\n::: {.cell}\n\n```{.r .cell-code}\n# Validate the model\nvalidate(fit_rms)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n          index.orig training    test optimism index.corrected  n\nDxy           0.5303   0.5365  0.5258   0.0107          0.5196 40\nR2            0.2726   0.2792  0.2676   0.0116          0.2609 40\nIntercept     0.0000   0.0000 -0.0149   0.0149         -0.0149 40\nSlope         1.0000   1.0000  0.9646   0.0354          0.9646 40\nEmax          0.0000   0.0000  0.0104   0.0104          0.0104 40\nD             0.2243   0.2305  0.2197   0.0109          0.2134 40\nU            -0.0009  -0.0009  0.0003  -0.0013          0.0003 40\nQ             0.2252   0.2315  0.2193   0.0121          0.2131 40\nB             0.1879   0.1865  0.1890  -0.0024          0.1903 40\ng             1.2470   1.2745  1.2332   0.0413          1.2057 40\ngp            0.2538   0.2567  0.2513   0.0054          0.2484 40\n```\n:::\n:::\n\n\nHow do we interpret this output? The `validate()` function applies resampling, which is a method to account for overfitting, with the results being printed in the 'index.corrected' column. However, if we are just interested in the apparent performance, we can just look at 'index.orig'. To calculate the C-statistic, we can use $c = \\frac{Dxy}{2} + 0.5$. In our case, the C-statistic would be:\n\n::: {.cell}\n\n```{.r .cell-code}\n# Calculate C-statistic\n0.5390 / 2 + 0.5\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] 0.7695\n```\n:::\n:::\n\n\nThe intercept and slope are given under 'Intercept' and 'Slope'. To get the calibration plot, we can use the `calibrate()` function (which aims to give overfitting-corrected estimates):\n\n::: {.cell}\n\n```{.r .cell-code}\n# Get calibration data\nplot(calibrate(fit_rms))\n```\n\n::: {.cell-output-display}\n![](pint_prd1_files/figure-html/unnamed-chunk-18-1.png){width=672}\n:::\n\n::: {.cell-output .cell-output-stdout}\n```\n\nn=2159   Mean absolute error=0.003   Mean squared error=2e-05\n0.9 Quantile of absolute error=0.008\n```\n:::\n:::\n\n\n## Method I or Method II?\nSo why would we pick one method over the other? Well, both methods have their advantages and disadvantages. It may be clear that `{rms}` has more native (i.e. inherent in the package) support for prediction modelling in the terms of validation with functions such as `validate()` and `calibate()`. Additionally, it makes it easy to get optimism-corrected performance measures. However, `{rms}` forces you to keep using the functions from its package, which are not always user intuitive. Moreover, only a few packages have been created as add-ons to `{rms}`, which makes it difficult to integrate into other analyses, or analyses that were not implemented/supported by the author. Conversely, `glm()` is part of the `{stats}` package, one of the base packages of R. A huge amount of packages have been developed which built on results from `glm()`. Additionally, as a base function of R, there is more focus on intuitive use and error-proofing.\n\n## The curse of the model fit object\nWhether we develop a prediction model using `glm()` or `lrm()`, functions such as `predict()`, `validate()`, and `calibrate()`, require us to supply the model fit object: the R object which contains the fitted model. This makes it much easier for the developer of the prediction model (who has the model fit object) to determine performance. However, validation becomes much more difficult. Either the validator needs to be supplied with the model fit object (meaning a certain level of skill in the corresponding programming language of the model fit object is required), or the validator needs to use functions that do not require the model fit.\n\nBecause the model fit object is often not supplied, the validator has to resot to the functions that do not require the model fit. For the C-statistic, this could be `roc()` from the `{pROC}` package. However, to get the linear predictors, the modeller could use `predict()`, but the validator will have to calculate these based on the coefficients. This can mean a lot of (unnecessary) manual work, including the risk of error when writing down the predictor coefficients.\n\n# Predicting survival in advanced lung cancer\nWe will now also develop a survival prediction model based on Cox regression using the lung dataset. We will use the `{survival}` package, which is part of the core R packages and which is closely related to the `glm()` function. As a second method, we will use the `cph()` function from `{rms}` package.\n\n## Getting to know our data\nWe (again hypothetically) consulted some lung cancer experts, who told us that the most important predictors we want to include are age ('age'), biological sex ('sex'), and weight loss in the last six months ('wt.loss') and that we should predict death within two years. Because weight loss had some missings, we used single imputation. Again, given the didactic purpose of this practical, this is fine, but in a real research setting, multiple imputation should be used.\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Single imputation of dat\nlung <- complete(mice(lung, m = 1))\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n\n iter imp variable\n  1   1  inst  ph.ecog  ph.karno  pat.karno  meal.cal  wt.loss\n  2   1  inst  ph.ecog  ph.karno  pat.karno  meal.cal  wt.loss\n  3   1  inst  ph.ecog  ph.karno  pat.karno  meal.cal  wt.loss\n  4   1  inst  ph.ecog  ph.karno  pat.karno  meal.cal  wt.loss\n  5   1  inst  ph.ecog  ph.karno  pat.karno  meal.cal  wt.loss\n```\n:::\n:::\n\n\nAdditionally, some survival times go beyond two years, so let's cap those survival times.\n\n::: {.cell}\n\n```{.r .cell-code}\n# Cap survival times to two years\nlung <- lung %>%\n    # Change variables\n    mutate(# Set status to censored if we observed individuals longer than two years \n           status = ifelse(time > 365.25 * 2, 1, status),      # <1>\n           # Cap time to two years\n           time = ifelse(time > 365.25 * 2, 365.25 * 2, time))  # <2>\n```\n:::\n\n\n1. If individuals died after two years, we did not observe an event within two years, therefore they become censored. Two years is defined by `365.25 * 2`.\n::: {.column-margin}\nNote that we use 365.25 days in a year. Although formally this should be [365.24](https://en.wikipedia.org/wiki/Year#Calendar_year), the second decimal is negligible for most follow-up durations in medical studies. \n:::\n\n2. We change the time until event to a maximum of two years. If the time is already shorter than two years, we keep that time.\n\nThe degree to which a prediction model is likely not overfitted depends in part on the number of cases and non-cases:\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# First, recode status to 0 and 1\nlung[[\"status\"]] <- lung[[\"status\"]] - 1\n\n# Counts of non-cases and cases\ntable(lung[[\"status\"]])\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n\n  0   1 \n 69 159 \n```\n:::\n\n```{.r .cell-code}\n# Distribution of non-cases and cases (as %)\nproportions(table(lung[[\"status\"]])) * 100\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n\n       0        1 \n30.26316 69.73684 \n```\n:::\n:::\n\n\nIn the lung dataset, we can see that we have more cases than non-cases. In this case, we look at the number of non-cases to get an idea of the risk of overfitting. Given the already small sample size and the small number of non-cases (n = 69, 30.3%), we likely have a high risk of overfitting. Given the scope of this practical, we will leave that be for now, but recommend that in real research settings, you evaluate this risk and carefully choose a path forward.\n\nBecause age and weight loss are continuous, we should investigate whether adding them as a linear term is sufficient or if we should model them non-linearly. Instead of the predicted probability, we plot the log relative hazard as a function of the predictor, using a function from `{Hmisc}`, which combines functions from `{rms}` and `{survival}`.\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Model the outcome as a function of age\ndat_plot <- do.call(\"cbind\", rcspline.plot(x = lung[[\"age\"]], y = lung[[\"time\"]], noprint = TRUE,\n                                           event = lung[[\"status\"]], model = \"cox\", statloc = \"none\")) %>%\n    # Change to data frame\n    as.data.frame()\n```\n:::\n\n\nAlthough the function already gives us a plot, the author of this practical is of the opinion that it is quite an ugly plot, so we will plot it ourselves:\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Get plot\nggplot(dat_plot, aes(x = x, y = V3)) +\n    # Geometries\n    geom_ribbon(aes(ymin = V4, ymax = V5), fill = \"#785EF0\", alpha = 0.2) +\n    geom_line(colour = \"#785EF0\") +\n    # Scaling\n    scale_y_continuous(limits = c(min(dat_plot[[\"V4\"]]) - 1, max(dat_plot[[\"V5\"]]) + 1), \n                       name = \"Log relative hazard\") +\n    # Labelling\n    xlab(\"Age (yrs)\") +\n    # Aesthetics\n    theme_bw()\n```\n\n::: {.cell-output-display}\n![](pint_prd1_files/figure-html/unnamed-chunk-23-1.png){width=672}\n:::\n:::\n\n\nNow we repeat this for weight loss.\n\n::: {.cell}\n\n```{.r .cell-code}\n# Model the outcome as a function of age\ndat_plot <- do.call(\"cbind\", rcspline.plot(x = lung[[\"wt.loss\"]], y = lung[[\"time\"]], noprint = TRUE,\n                                           event = lung[[\"status\"]], model = \"cox\", statloc = \"none\")) %>%\n    # Change to data frame\n    as.data.frame()\n```\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\n# Get plot\nggplot(dat_plot, aes(x = x, y = V3)) +\n    # Geometries\n    geom_ribbon(aes(ymin = V4, ymax = V5), fill = \"#785EF0\", alpha = 0.2) +\n    geom_line(colour = \"#785EF0\") +\n    # Scaling\n    scale_y_continuous(limits = c(min(dat_plot[[\"V4\"]]) - 1, max(dat_plot[[\"V5\"]]) + 1), \n                       name = \"Log relative hazard\") +\n    # Labelling\n    xlab(\"Weight loss in last 6 months (pounds)\") +\n    # Aesthetics\n    theme_bw()\n```\n\n::: {.cell-output-display}\n![](pint_prd1_files/figure-html/unnamed-chunk-25-1.png){width=672}\n:::\n:::\n\n\nAlthough for age a linear association is fine, we will need some kind of non-linear transformation for weight loss to accurately capture it in the prediction model. \n\n## Method I: coxph()\nTo develop the prediction model, we will use the `coxph()` function from the `{survival}` package. This works just like `glm()` in how we specify the prediction model, except that we specify the outcome using the `Surv()` function.\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Develop the prediction model\nfit <- coxph(Surv(time, status) ~                                             # <1>\n               age +                                             # <2>\n               sex +\n               ns(wt.loss, knots = c(0, 17)),                                         # <3>\n           data = lung)                       \n```\n:::\n\n\n1. Fit a Cox regression model, with the outcome being a survival object containing the event and time to event.\n2. Add predictors age (linearly) and sex.\n3. Add weight loss as a natural cubic spline using the `ns()` function from `{splines}`. We put knots at 0 and 17 based on the previous graph.\n\nLet's see what the output looks like:\n\n::: {.cell}\n\n```{.r .cell-code}\n# Print output\nsummary(fit)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nCall:\ncoxph(formula = Surv(time, status) ~ age + sex + ns(wt.loss, \n    knots = c(0, 17)), data = lung)\n\n  n= 228, number of events= 159 \n\n                                    coef exp(coef)  se(coef)      z Pr(>|z|)   \nage                             0.017326  1.017477  0.009467  1.830  0.06724 . \nsex                            -0.543317  0.580819  0.171707 -3.164  0.00156 **\nns(wt.loss, knots = c(0, 17))1 -0.197963  0.820401  0.495679 -0.399  0.68962   \nns(wt.loss, knots = c(0, 17))2 -2.448763  0.086400  1.732283 -1.414  0.15748   \nns(wt.loss, knots = c(0, 17))3 -1.036500  0.354694  0.963225 -1.076  0.28190   \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n                               exp(coef) exp(-coef) lower .95 upper .95\nage                               1.0175     0.9828  0.998771    1.0365\nsex                               0.5808     1.7217  0.414842    0.8132\nns(wt.loss, knots = c(0, 17))1    0.8204     1.2189  0.310530    2.1674\nns(wt.loss, knots = c(0, 17))2    0.0864    11.5740  0.002897    2.5765\nns(wt.loss, knots = c(0, 17))3    0.3547     2.8193  0.053698    2.3429\n\nConcordance= 0.605  (se = 0.025 )\nLikelihood ratio test= 17.6  on 5 df,   p=0.003\nWald test            = 16.69  on 5 df,   p=0.005\nScore (logrank) test = 17.14  on 5 df,   p=0.004\n```\n:::\n:::\n\nWe see that weight loss has three coefficients, one for the lowest value through 0 (1), one for 0 through 10 (2), and one for 10 through the highest value (3). Also note that we do not have an intercept: to calculate the final predicted risk, we need to use the baseline hazard instead of the intercept. However, the baseline hazard is different for each timepoint.\n\nWe can use `basehaz()` to calculate the baseline hazard at our timepoint of interest (2 years):\n\n::: {.cell}\n\n```{.r .cell-code}\n# Retrieve baseline hazard at 2 years\nfilter(basehaz(fit), time == 365.25 * 2)[[\"hazard\"]]\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] 2.143102\n```\n:::\n:::\n\n\nDiscrimination of this model can be calculated using Harrell's C-statistic:\n* The function `cindex()` from the `{dynpred}` package (does not require model fit, but does require development\nformula).\n* The function `rcorr.cens()` from the `{Hmisc}` package (does not require model fit, but may be slow).\n* The function `cIndex()` from the `{intsurv}` package.\n\nThe calibration slope can be calculated as before, using `coxph()` and a calibration can also be drawn using the same methods as discussed before. In the case of competing risks, take note of validation measures for competing risk settings. More on that can be found in [Van Geloven *et al.* 2021](https://doi.org/10.1136/bmj-2021-069249) and [Ramspek *et al.* 2021](https://doi.org/10.1093/ije/dyab256).\n\n## Method II: cph()\nWe can also use `cph()` from `{rms}`. Although this is an extension of `coxph()` from `{survival}`, the returned model fit object can be used with the `validate()` and `calibrate()` functions from `{rms}`:\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Develop the prediction model\nfit <- cph(Surv(time, status) ~                                 \n               age +                                            \n               sex +\n               rcs(wt.loss),                                         # <1>\n           data = lung)                       \n```\n\n::: {.cell-output .cell-output-stderr}\n```\nnumber of knots in rcs defaulting to 5\n```\n:::\n:::\n\n\n1. This time, we use `rcs()` from `{rms}` instead of `ns()` from `{splines}`. The advantage of `rcs()` is that it does not transform the underlying variable (i.e. not a B-spline matrix basis for a natural cubic spline) and is therefore easier to interpret and write down in a concise formula. However, we are forced to use at least 3 knots meaning more coefficients are estimated, whilst `ns()` allows us to use a single knot. Note that we can both use `rcs()` and `ns()` in both `coxph()` and `cph()`.\n\n## The curse of the model fit object revisited\nWe already saw that with the model fit object, validation of a prediction model becomes quite easy.\n\n::: {.callout-important}\n## Using `predict()`\nIf we use `predict()` to calculate the survival probability for an individual based on a Cox regression model (thus using `predict(fit, type = \"survival\")`, note that the survival probability is not the probability at our specified time horizon, but at the end of their observed follow-up. Therefore, we cannot use this to calculate predictions (see also [this issue](https://github.com/therneau/survival/issues/251) opened by the author of this practical, who turned out to be wrong but learned along the way)\n\nTo get individual predictions, instead use `predict(fit, type = \"lp\")` and calculate the survival probability from the linear predictor using $\\hat{y}(t) = 1 - S_0(t)^{e^{LP_i}}$ where $S_0(t) = e^{-H_0(t)}$ and $H_0(t)$ is the baseline hazard. \n:::\n\nHowever, in the absence of a model fit object, the validator may struggle more, especially for Cox regression models. By default, both `coxph()` and `cph()` estimate a centered Cox regression model. This means that the reference individual for the model is not someone without any binary predictor and with all continuous variables at 0, but someone without any binary predictor and with all continuous variables at the mean of that variable. If we then want to calculate an individual's linear predictor, we also should have the linear predictor of the sample ($LP_{sample}$). To calculate an individual's linear predictor for the final prediction, we substract $LP_{sample}$ from their individual linear predictor $LP_{individual}$: $\\hat{y}(t) = 1 - S_0(t)^{e^{LP_{individual} - LP_{sample}}}$. More on this can be found in [this blogpost](https://missingdatasolutions.rbind.io/2022/12/cox-baseline-hazard/).\n\n# Final words\nWe have now taken a look at the basics for prediction modelling in R. Although many more packages are available for our goals (e.g. `{riskRegression`), we introduced the two most commonly used packages. If you start developing or validating a prediction model on your own, look back to this practical and never be afraid to ask, whether that be an experienced modeller, the internet, or (with due caution applied) a large language model.\n\n# Postscript\n\nThis practical was developed on:\n\n\n::: {.cell}\n::: {.cell-output .cell-output-stdout}\n```\nR version 4.4.1 (2024-06-14 ucrt)\nPlatform: x86_64-w64-mingw32/x64\nRunning under: Windows 11 x64 (build 22631)\n\nMatrix products: default\n\n\nlocale:\n[1] LC_COLLATE=English_United Kingdom.utf8 \n[2] LC_CTYPE=English_United Kingdom.utf8   \n[3] LC_MONETARY=English_United Kingdom.utf8\n[4] LC_NUMERIC=C                           \n[5] LC_TIME=English_United Kingdom.utf8    \n\ntime zone: Europe/Amsterdam\ntzcode source: internal\n\nattached base packages:\n[1] splines   stats     graphics  grDevices utils     datasets  methods  \n[8] base     \n\nother attached packages:\n [1] ggExtra_0.10.1    ggplot2_3.5.1     mice_3.16.0       survival_3.6-4   \n [5] rms_6.8-2         Hmisc_5.1-3       pROC_1.18.5       DescTools_0.99.56\n [9] rio_1.2.1         magrittr_2.0.3    dplyr_1.1.4      \n\nloaded via a namespace (and not attached):\n  [1] gridExtra_2.3      gld_2.6.6          sandwich_3.1-1    \n  [4] readxl_1.4.3       rlang_1.1.4        multcomp_1.4-26   \n  [7] e1071_1.7-14       polspline_1.1.25   compiler_4.4.1    \n [10] mgcv_1.9-1         vctrs_0.6.5        quantreg_5.98     \n [13] stringr_1.5.1      pkgconfig_2.0.3    shape_1.4.6.1     \n [16] fastmap_1.2.0      backports_1.5.0    labeling_0.4.3    \n [19] utf8_1.2.4         promises_1.3.0     rmarkdown_2.27    \n [22] nloptr_2.1.1       MatrixModels_0.5-3 purrr_1.0.2       \n [25] xfun_0.46          glmnet_4.1-8       jomo_2.7-6        \n [28] jsonlite_1.8.8     later_1.3.2        pan_1.9           \n [31] broom_1.0.6        cluster_2.1.6      R6_2.5.1          \n [34] stringi_1.8.4      boot_1.3-30        rpart_4.1.23      \n [37] cellranger_1.1.0   Rcpp_1.0.13        iterators_1.0.14  \n [40] knitr_1.48         zoo_1.8-12         R.utils_2.12.3    \n [43] base64enc_0.1-3    pacman_0.5.1       httpuv_1.6.15     \n [46] Matrix_1.7-0       nnet_7.3-19        tidyselect_1.2.1  \n [49] rstudioapi_0.16.0  yaml_2.3.10        codetools_0.2-20  \n [52] miniUI_0.1.1.1     lattice_0.22-6     tibble_3.2.1      \n [55] plyr_1.8.9         shiny_1.9.1        withr_3.0.1       \n [58] evaluate_0.24.0    foreign_0.8-86     proxy_0.4-27      \n [61] pillar_1.9.0       checkmate_2.3.2    foreach_1.5.2     \n [64] generics_0.1.3     munsell_0.5.1      scales_1.3.0      \n [67] rootSolve_1.8.2.4  minqa_1.2.7        xtable_1.8-4      \n [70] class_7.3-22       glue_1.7.0         lmom_3.0          \n [73] tools_4.4.1        data.table_1.15.4  lme4_1.1-35.5     \n [76] SparseM_1.84-2     Exact_3.3          mvtnorm_1.3-1     \n [79] grid_4.4.1         tidyr_1.3.1        colorspace_2.1-1  \n [82] nlme_3.1-164       htmlTable_2.4.3    Formula_1.2-5     \n [85] cli_3.6.3          fansi_1.0.6        expm_1.0-0        \n [88] gtable_0.3.5       R.methodsS3_1.8.2  digest_0.6.36     \n [91] TH.data_1.1-2      farver_2.1.2       htmlwidgets_1.6.4 \n [94] R.oo_1.26.0        htmltools_0.5.8.1  lifecycle_1.0.4   \n [97] httr_1.4.7         mitml_0.4-5        mime_0.12         \n[100] MASS_7.3-60.2     \n```\n:::\n:::\n",
    "supporting": [
      "pint_prd1_files"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}